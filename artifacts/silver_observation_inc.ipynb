{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {},
      "source": [
        "bronze_location = \"abfss://bronze@prsynapselab.dfs.core.windows.net/\"\r\n",
        "write_mode=\"append\"\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\r\n",
        "val write_mode=\"append\"\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "observationSchema = spark.read.json(bronze_location+\"historic_data/observation/year=2016/part-00000-680e113d-60c6-468f-ba37-ea5ffd426dfe.c000.json\").schema\r\n",
        "# observationSchema = spark.read.json(bronze_location+\"incremental_data/observation/year=2021/month=1/day=1/part-00000-6aadddce-f3ce-40c6-a5d0-d5f36d8fef75.c000.json\").schema\r\n",
        "observation_df = spark.read.schema(observationSchema).json(bronze_location+\"incremental_data/observation/*/*/*/*.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "observation_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "display(observation_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "observation_df_selected = observation_df.select(\"id\",\"resourceType\",\"status\",\"issued\",\"subject\",\"encounter\",\"effectiveDateTime\",\"valueQuantity\",\"valueString\",\"code\",\"component\",\"category\") \\\r\n",
        "                        .toDF(*(\"observation_id\",\"resourceType\",\"status\",\"issued\",\"patient\",\"encounter\",\"effectiveDateTime\",\"valueQuantity\",\"valueString\",\"code\",\"component\",\"category\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "observation_df_selected.createOrReplaceTempView(\"observation_df_selected\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\r\n",
        "import org.apache.spark.sql.types.{StructType,ArrayType}\r\n",
        "import org.apache.spark.sql.DataFrame\r\n",
        "import org.apache.spark.sql.Column\r\n",
        "import org.apache.spark.sql.functions.col\r\n",
        "import org.apache.spark.sql.functions.explode_outer\r\n",
        "\r\n",
        "def flattenDataFrame(df: DataFrame): DataFrame = {\r\n",
        "  val fields = df.schema.fields\r\n",
        "  val fieldNames = fields.map(x => x.name)\r\n",
        "\r\n",
        "  for (i <- fields.indices) {\r\n",
        "    val field = fields(i)\r\n",
        "    val fieldType = field.dataType\r\n",
        "    val fieldName = field.name\r\n",
        "    fieldType match {\r\n",
        "      case _: ArrayType =>\r\n",
        "        val fieldNamesExcludingArray = fieldNames.filter(_ != fieldName)\r\n",
        "        val fieldNamesAndExplode = fieldNamesExcludingArray ++ Array(\r\n",
        "          s\"explode_outer($fieldName) as $fieldName\"\r\n",
        "        )\r\n",
        "        val explodedDf = df.selectExpr(fieldNamesAndExplode: _*)\r\n",
        "        return flattenDataFrame(explodedDf)\r\n",
        "      case structType: StructType =>\r\n",
        "        val childFieldNames =\r\n",
        "          structType.fieldNames.map(childname => fieldName + \".\" + childname)\r\n",
        "        val newFieldNames = fieldNames.filter(_ != fieldName) ++ childFieldNames\r\n",
        "        import org.apache.spark.sql.functions.col\r\n",
        "\r\n",
        "        val renamedCols =\r\n",
        "          newFieldNames.map { x =>\r\n",
        "            col(x.toString).as(x.toString.replace(\".\", \"_\"))\r\n",
        "          }\r\n",
        "\r\n",
        "        val explodedDf = df.select(renamedCols: _*)\r\n",
        "        return flattenDataFrame(explodedDf)\r\n",
        "      case _ =>\r\n",
        "    }\r\n",
        "  }\r\n",
        "\r\n",
        "  df\r\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Creating observation Main Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "observation_df_selected.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\r\n",
        "val observation_main_df = spark.sql(\"select * from observation_df_selected\").drop(\"code\",\"component\",\"category\");\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\r\n",
        "val observation_main_df_flattened = flattenDataFrame(observation_main_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        },
        "collapsed": false
      },
      "source": [
        "%%spark\r\n",
        "display(observation_main_df_flattened)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\r\n",
        "val silver_location = \"abfss://silver@prsynapselab.dfs.core.windows.net/\"\r\n",
        "val write_mode=\"append\"\r\n",
        "\r\n",
        "observation_main_df_flattened.coalesce(200).write.format(\"delta\").option(\"path\", silver_location+\"observation_main\").mode(write_mode).saveAsTable(\"fhir.observation_main_hash\")\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Creating observation Code Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        },
        "collapsed": false
      },
      "source": [
        "%%spark\r\n",
        "val observation_code_df = spark.sql(\"select observation_id, code from observation_df_selected\")\r\n",
        "val observation_code_df_flattened = flattenDataFrame(observation_code_df)\r\n",
        "display(observation_code_df_flattened)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\r\n",
        "val silver_location = \"abfss://silver@prsynapselab.dfs.core.windows.net/\"\r\n",
        "val write_mode=\"append\"\r\n",
        "\r\n",
        "observation_code_df_flattened.coalesce(200).write.format(\"delta\").option(\"path\", silver_location+\"observation_code\").mode(write_mode).saveAsTable(\"fhir.observation_code_hash\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Creating observation component Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        },
        "collapsed": false
      },
      "source": [
        "%%spark\r\n",
        "val observation_component_df = spark.sql(\"select observation_id, component from observation_df_selected\")\r\n",
        "val observation_component_df_flattened = flattenDataFrame(observation_component_df)\r\n",
        "display(observation_component_df_flattened)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\r\n",
        "val silver_location = \"abfss://silver@prsynapselab.dfs.core.windows.net/\"\r\n",
        "val write_mode=\"append\"\r\n",
        "\r\n",
        "observation_component_df_flattened.coalesce(200).write.format(\"delta\").option(\"path\", silver_location+\"observation_component\").mode(write_mode).saveAsTable(\"fhir.observation_component_hash\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Creating observation category Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        },
        "collapsed": false
      },
      "source": [
        "%%spark\r\n",
        "val observation_category_df = spark.sql(\"select observation_id, category from observation_df_selected\")\r\n",
        "val observation_category_df_flattened = flattenDataFrame(observation_category_df)\r\n",
        "display(observation_category_df_flattened)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\r\n",
        "val silver_location = \"abfss://silver@prsynapselab.dfs.core.windows.net/\"\r\n",
        "val write_mode=\"append\"\r\n",
        "\r\n",
        "observation_category_df_flattened.coalesce(200).write.format(\"delta\").option(\"path\", silver_location+\"observation_category\").mode(write_mode).saveAsTable(\"fhir.observation_category_hash\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "for some reason my main table failed to register in previous step.   \r\n",
        "```\r\n",
        "%%sql\r\n",
        "\r\n",
        "CREATE TABLE fhir.observation_main_hash USING DELTA LOCATION 'abfss://silver@prsynapselab.dfs.core.windows.net/observation_main'\r\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}