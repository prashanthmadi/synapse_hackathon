{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pyspark.sql.functions as f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "observationSchema = spark.read.json(\"abfss://raw1@prsynapselab.dfs.core.windows.net/000047ca-00c7-492b-bf65-740805144cd2/Observation.ndjson\").schema\r\n",
        "observation_df = spark.read.schema(observationSchema).json(\"abfss://raw1@prsynapselab.dfs.core.windows.net/*/Observation.ndjson\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "observation_2016_2020 = observation_df.withColumn(\"year\",f.year(f.col('issued'))).filter(f.col(\"year\").between(2016,2020))\r\n",
        "observation_2016_2020.coalesce(3000).write.partitionBy(\"year\").json(\"abfss://bronze@prsynapselab.dfs.core.windows.net/historic_data/observation/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "observation_2021 = observation_df.withColumn(\"year\",f.year(f.col('issued'))).withColumn(\"month\",f.month(f.col('issued'))).withColumn(\"day\",f.dayofmonth(f.col('issued'))).filter(f.col(\"year\") == 2021)\r\n",
        "observation_2021.coalesce(3000).write.partitionBy(\"year\",\"month\",\"day\").json(\"abfss://bronze@prsynapselab.dfs.core.windows.net/incremental_data/observation/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "claimSchema = spark.read.json(\"abfss://raw1@prsynapselab.dfs.core.windows.net/000047ca-00c7-492b-bf65-740805144cd2/Claim.ndjson\").schema\r\n",
        "claim_df = spark.read.schema(claimSchema).json(\"abfss://raw1@prsynapselab.dfs.core.windows.net/*/Claim.ndjson\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "claim_2016_2020 = claim_df.withColumn(\"year\",f.year(f.col('created'))).filter(f.col(\"year\").between(2016,2020))\r\n",
        "claim_2016_2020.coalesce(3000).write.partitionBy(\"year\").json(\"abfss://bronze@prsynapselab.dfs.core.windows.net/historic_data/claim/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "claim_2021 = claim_df.withColumn(\"year\",f.year(f.col('created'))).withColumn(\"month\",f.month(f.col('created'))).withColumn(\"day\",f.dayofmonth(f.col('created'))).filter(f.col(\"year\") == 2021)\r\n",
        "claim_2021.coalesce(3000).write.partitionBy(\"year\",\"month\",\"day\").json(\"abfss://bronze@prsynapselab.dfs.core.windows.net/incremental_data/claim/\")"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}